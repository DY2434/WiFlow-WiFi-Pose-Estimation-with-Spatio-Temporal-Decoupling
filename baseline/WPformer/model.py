import numpy as np
import csv
import pandas as pd
import pickle
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
import scipy.io as scio
import os.path
import torch.nn as nn
from torch.autograd import Variable
from evaluation import compute_pck_pckh_15
import time
from torchvision.transforms import Resize
from ChannelTrans import ChannelTransformer
import cv2
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from tqdm import tqdm
import random
import os
import torchvision
from torchvision.models import ResNet34_Weights  # Add this import
import argparse
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import glob
import hdf5storage

# ============================== #
# è¦ä¿ç•™çš„å…³é”®ç‚¹å®šä¹‰
# ============================== #
KEEP_KEYPOINTS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
KEYPOINT_MAPPING = {old_idx: new_idx for new_idx, old_idx in enumerate(KEEP_KEYPOINTS)}


# ============================== #
# æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨ä½ æä¾›çš„æ•°æ®åŠ è½½å™¨
# ============================== #
class PreprocessedCSIKeypointsDataset(Dataset):
    """ä½¿ç”¨é¢„å¤„ç†åçš„CSIæ•°æ®å’ŒPAMæ ¼å¼æ ‡ç­¾çš„æ•°æ®é›†ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""

    def __init__(self, csi_data_dir, pam_label_dir, keypoint_scale=1000.0, transform=None, enable_zero_clean=True):
        self.csi_windows = np.load(os.path.join(csi_data_dir, "csi_windows.npy"))

        window_info = np.load(os.path.join(csi_data_dir, "window_info.npz"))
        self.window_to_file = window_info['window_to_file']
        self.window_to_frame = window_info['window_to_frame']

        file_info = np.load(os.path.join(csi_data_dir, "file_info.npz"), allow_pickle=True)
        self.file_ids = file_info['file_ids']
        self.window_ranges = file_info['window_ranges']

        config = np.load(os.path.join(csi_data_dir, "config.npz"))
        self.window_size = config['window_size']
        self.stride = config['stride']

        self.keypoint_scale = keypoint_scale
        self.transform = transform
        self.pam_label_dir = pam_label_dir
        self.enable_zero_clean = enable_zero_clean

        # ç¼“å­˜å•ä¸ªPAMå¸§ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªåºåˆ—ï¼‰
        self._pam_cache = {}
        self._cache_size = 100  # å¢åŠ ç¼“å­˜å¤§å°

        print(f"åŠ è½½äº† {len(self.csi_windows)} ä¸ªCSIçª—å£")
        print(f"PAMæ ‡ç­¾ç›®å½•: {pam_label_dir}")
        print(f"æ–‡ä»¶IDæ•°é‡: {len(self.file_ids)}")
        print(f"é›¶å€¼æ¸…ç†: {'å¯ç”¨(å•å¸§å‡å€¼æ³•)' if enable_zero_clean else 'ç¦ç”¨'}")

    def _get_pam_file_path(self, file_idx, frame_idx):
        """æ ¹æ®æ–‡ä»¶ç´¢å¼•å’Œå¸§ç´¢å¼•è·å–PAMæ–‡ä»¶è·¯å¾„"""
        file_id = self.file_ids[file_idx]
        pam_filename = f"{file_id}_dual_cropped_frame_{frame_idx:06d}.mat"

        for person_idx in range(1, 6):
            person_dir = os.path.join(self.pam_label_dir, f"wisppn_labels{person_idx}")
            pam_path = os.path.join(person_dir, pam_filename)
            if os.path.exists(pam_path):
                return pam_path
        return None

    def _clean_single_frame_zeros(self, keypoint):
        """æ¸…ç†å•å¸§ä¸­çš„é›¶å€¼å…³é”®ç‚¹ï¼ˆå•å¸§å‡å€¼æ³•ï¼‰"""
        cleaned = keypoint.copy()

        # æ‰¾åˆ°éé›¶å…³é”®ç‚¹çš„æ©ç 
        non_zero_mask = (keypoint[:, 0] != 0) | (keypoint[:, 1] != 0)

        # åªæœ‰å½“å­˜åœ¨è‡³å°‘ä¸€ä¸ªéé›¶ç‚¹æ—¶æ‰è¿›è¡Œå¤„ç†
        if non_zero_mask.any():
            # è®¡ç®—éé›¶å…³é”®ç‚¹çš„å¹³å‡ä½ç½®
            mean_pos = keypoint[non_zero_mask].mean(axis=0)
            # æ‰¾åˆ°æ‰€æœ‰é›¶å€¼ç‚¹çš„ç´¢å¼•
            zero_indices = np.where(~non_zero_mask)[0]

            # ç”¨å‡å€¼æ›¿æ¢é›¶å€¼ç‚¹
            for idx in zero_indices:
                cleaned[idx] = mean_pos

        return cleaned

    def _load_pam_frame(self, file_idx, frame_idx):
        """åŠ è½½ç‰¹å®šå¸§çš„PAMæ•°æ®ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        cache_key = f"{file_idx}_{frame_idx}"

        if cache_key in self._pam_cache:
            return self._pam_cache[cache_key]

        # ç¼“å­˜ç®¡ç† - LRUç­–ç•¥
        if len(self._pam_cache) >= self._cache_size:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self._pam_cache))
            del self._pam_cache[oldest_key]

        # è·å–PAMæ–‡ä»¶è·¯å¾„
        pam_path = self._get_pam_file_path(file_idx, frame_idx)

        if pam_path is None:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›é›¶çŸ©é˜µ
            pam_data = np.zeros((3, 15, 15), dtype=np.float32)
        else:
            try:
                # åŠ è½½.matæ–‡ä»¶
                data = hdf5storage.loadmat(pam_path)
                pam_data = data['jointsMatrix'][:3, :, :].astype(np.float32)

                # å¦‚æœå¯ç”¨æ¸…ç†ï¼Œåº”ç”¨å•å¸§å‡å€¼æ¸…ç†
                if self.enable_zero_clean:
                    # æå–å…³é”®ç‚¹åæ ‡
                    keypoints = np.zeros((15, 2), dtype=np.float32)
                    for kp_idx in range(15):
                        keypoints[kp_idx, 0] = pam_data[0, kp_idx, kp_idx]
                        keypoints[kp_idx, 1] = pam_data[1, kp_idx, kp_idx]

                    # æ¸…ç†é›¶å€¼
                    cleaned_keypoints = self._clean_single_frame_zeros(keypoints)

                    # æ›´æ–°PAMçŸ©é˜µ
                    for kp_idx in range(15):
                        # æ›´æ–°å¯¹è§’çº¿ï¼ˆç»å¯¹åæ ‡ï¼‰
                        pam_data[0, kp_idx, kp_idx] = cleaned_keypoints[kp_idx, 0]
                        pam_data[1, kp_idx, kp_idx] = cleaned_keypoints[kp_idx, 1]

                    # æ›´æ–°éå¯¹è§’çº¿ï¼ˆç›¸å¯¹åæ ‡ï¼‰
                    for i in range(15):
                        for j in range(15):
                            if i != j:
                                pam_data[0, i, j] = cleaned_keypoints[i, 0] - cleaned_keypoints[j, 0]
                                pam_data[1, i, j] = cleaned_keypoints[i, 1] - cleaned_keypoints[j, 1]

                # å½’ä¸€åŒ–åæ ‡
                pam_data[0:2, :, :] = pam_data[0:2, :, :] / self.keypoint_scale
                # ç½®ä¿¡åº¦ä¿æŒä¸å˜

            except Exception as e:
                print(f"åŠ è½½PAMæ–‡ä»¶å¤±è´¥ {pam_path}: {e}")
                pam_data = np.zeros((3, 15, 15), dtype=np.float32)

        # ç¼“å­˜ç»“æœ
        self._pam_cache[cache_key] = pam_data
        return pam_data

    def __len__(self):
        return len(self.csi_windows)

    def __getitem__(self, idx):
        # è·å–CSIçª—å£
        csi_window = self.csi_windows[idx]

        # è·å–å¯¹åº”çš„æ–‡ä»¶å’Œå¸§ç´¢å¼•
        file_idx = self.window_to_file[idx]
        frame_idx = self.window_to_frame[idx]

        # æŒ‰éœ€åŠ è½½å•ä¸ªPAMå¸§
        pam_label = self._load_pam_frame(file_idx, frame_idx)

        # è½¬æ¢ä¸ºå¼ é‡
        csi_tensor = torch.from_numpy(csi_window).float()
        pam_tensor = torch.from_numpy(pam_label).float()

        if self.transform:
            csi_tensor = self.transform(csi_tensor)

        return csi_tensor, pam_tensor

    def get_file_indices(self):
        """è·å–æ‰€æœ‰æ–‡ä»¶ç´¢å¼•"""
        return list(range(len(self.file_ids)))

    def get_samples_from_file(self, file_idx):
        """è·å–æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•"""
        start_idx, end_idx = self.window_ranges[file_idx]
        return list(range(start_idx, end_idx))


def create_preprocessed_train_val_test_loaders(dataset, batch_size=64, num_workers=0, random_seed=42):
    """æŒ‰æ–‡ä»¶çº§åˆ«åˆ’åˆ†é¢„å¤„ç†åçš„æ•°æ®é›†"""
    # è®¾ç½®éšæœºç§å­
    random.seed(random_seed)

    # è·å–æ‰€æœ‰æ–‡ä»¶ç´¢å¼•
    file_indices = dataset.get_file_indices()
    total_files = len(file_indices)

    # éšæœºæ‰“ä¹±æ–‡ä»¶é¡ºåº
    random.shuffle(file_indices)

    # æŒ‰æ¯”ä¾‹åˆ’åˆ†æ–‡ä»¶
    train_ratio, val_ratio = 0.7, 0.15
    train_split = int(np.floor(train_ratio * total_files))
    val_split = int(np.floor(val_ratio * total_files))

    # è·å–æ¯ä¸ªé›†åˆçš„æ–‡ä»¶ç´¢å¼•
    train_file_indices = file_indices[:train_split]
    val_file_indices = file_indices[train_split:train_split + val_split]
    test_file_indices = file_indices[train_split + val_split:]

    # è·å–æ¯ä¸ªé›†åˆçš„æ ·æœ¬ç´¢å¼•
    train_indices = []
    val_indices = []
    test_indices = []

    for file_idx in train_file_indices:
        train_indices.extend(dataset.get_samples_from_file(file_idx))

    for file_idx in val_file_indices:
        val_indices.extend(dataset.get_samples_from_file(file_idx))

    for file_idx in test_file_indices:
        test_indices.extend(dataset.get_samples_from_file(file_idx))

    print(f"è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬ (æ¥è‡ª {len(train_file_indices)} ä¸ªæ–‡ä»¶)")
    print(f"éªŒè¯é›†: {len(val_indices)} æ ·æœ¬ (æ¥è‡ª {len(val_file_indices)} ä¸ªæ–‡ä»¶)")
    print(f"æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬ (æ¥è‡ª {len(test_file_indices)} ä¸ªæ–‡ä»¶)")

    # åˆ›å»ºå­é›†
    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)
    test_dataset = torch.utils.data.Subset(dataset, test_indices)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        pin_memory=True,
        num_workers=num_workers,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=4 if num_workers > 0 else None,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        pin_memory=True,
        num_workers=num_workers,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=4 if num_workers > 0 else None,
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        pin_memory=True,
        num_workers=num_workers,
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=4 if num_workers > 0 else None,
    )

    return train_loader, val_loader, test_loader


# ============================== #
# åŸå§‹çš„posenetæ¨¡å‹ - ä¿æŒä¸å˜ï¼Œåªåšè¾“å…¥è¾“å‡ºé€‚é…
# ============================== #
class posenet(nn.Module):
    """
    WiFi-based Human Pose Estimation Network (WPFormer)

    Input: WiFi CSI data from 6 antenna pairs
    Output: 2D pose landmarks (y coordinates for 15 keypoints)

    Modified Architecture:
    - Processes 6 CSI streams from antenna pairs
    - Uses shared ResNet34 encoder for feature extraction
    - Applies Channel Transformer for feature integration
    - Outputs 2Ã—15 pose coordinates via decoder and average pooling
    """

    def __init__(self):
        super(posenet, self).__init__()

        # Create ResNet34 encoder (shared weights for all 6 antenna pairs)
        # FIX: Use weights parameter instead of deprecated pretrained parameter
        # resnet_raw_model1 = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)
        # é¡¹ç›®ç›®å½•ä¸‹çš„æƒé‡æ–‡ä»¶è·¯å¾„
        project_weight_path = "./resnet34-b627a593.pth"  # ç›¸å¯¹è·¯å¾„
        # æˆ–è€…ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼š
        # project_weight_path = "/home/aip-dt-01/ä¸‹è½½/DY/resnet34-b627a593.pth"

        try:
            if os.path.exists(project_weight_path):
                print(f"å‘ç°é¡¹ç›®ç›®å½•ä¸‹çš„æƒé‡æ–‡ä»¶: {project_weight_path}")

                # åˆ›å»ºä¸å¸¦æƒé‡çš„ResNet34æ¨¡å‹
                resnet_raw_model1 = torchvision.models.resnet34(weights=None)

                # æ‰‹åŠ¨åŠ è½½æƒé‡
                print("æ­£åœ¨åŠ è½½æƒé‡...")
                state_dict = torch.load(project_weight_path, map_location='cpu')
                resnet_raw_model1.load_state_dict(state_dict)
                print("âœ… æˆåŠŸä»é¡¹ç›®ç›®å½•åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼")

            else:
                print(f"âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {project_weight_path}")
                print("è¯·ç¡®ä¿å°† resnet34-b627a593.pth æ–‡ä»¶æ”¾åˆ°é¡¹ç›®ç›®å½•ä¸‹")
                print("ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡...")
                resnet_raw_model1 = torchvision.models.resnet34(weights=None)

        except Exception as e:
            print(f"âŒ åŠ è½½æƒé‡å¤±è´¥: {e}")
            print("ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡...")
            resnet_raw_model1 = torchvision.models.resnet34(weights=None)

        # Expected feature map sizes at each ResNet layer
        filters = [64, 64, 128, 256, 512]

        # Encoder components from ResNet34 (shared across all 6 antenna pairs)
        # Input: 1-channel CSI data -> 64-channel feature maps
        self.encoder_conv1_p1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1,
                                          bias=False)
        self.encoder_bn1_p1 = resnet_raw_model1.bn1
        self.encoder_relu_p1 = resnet_raw_model1.relu

        # ResNet34 layers (Block 1-5 in paper's Table I)
        self.encoder_layer1_p1 = resnet_raw_model1.layer1  # Block 1: 64Ã—60Ã—32
        self.encoder_layer2_p1 = resnet_raw_model1.layer2  # Block 2: 128Ã—30Ã—16
        self.encoder_layer3_p1 = resnet_raw_model1.layer3  # Block 3: 256Ã—15Ã—8
        # self.encoder_layer4_p1 = resnet_raw_model1.layer4  # Block 4: 512Ã—15Ã—4

        # Channel Transformer for feature integration
        # Input: 512Ã—15Ã—24 (concatenated features from 6 antenna pairs)
        # Output: 512Ã—15Ã—24 (same size, with attention weights)
        self.tf = ChannelTransformer(vis=False, img_size=[15, 144], channel_num=256, num_layers=1, num_heads=3)

        # Decoder: 512Ã—15Ã—24 -> 2Ã—15Ã—24 (pose coordinates)
        self.decode = nn.Sequential(
            nn.Conv2d(256, 32, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 2, kernel_size=1, stride=1, padding=0, bias=False),  # 2 channels for (x,y) coordinates
            nn.BatchNorm2d(2),
            nn.ReLU(inplace=True)
        )

        # Batch normalization layers
        self.bn1 = nn.BatchNorm1d(2)  # For final 2D pose output
        self.bn2 = nn.BatchNorm2d(256)  # For concatenated features before transformer
        self.rl = nn.ReLU(inplace=True)

    def forward(self, x):
        """
        Forward pass through WPFormer network

        Args:
            x: Input CSI data, shape [batch_size, 540, 20]
               Modified to accept the new data format and convert it internally

        Returns:
            x: Pose landmarks, shape [batch_size, 15, 2] (15 keypoints with x,y coordinates)
            time_sum: Forward pass execution time
        """

        # è¾“å…¥é€‚é…ï¼šä» [batch_size, 540, 20] è½¬æ¢ä¸ºåŸå§‹æ¨¡å‹éœ€è¦çš„æ ¼å¼
        batch_size = x.size(0)

        # å°†540æ‹†åˆ†æˆ18ä¸ª30ï¼ˆæ¯ä¸ªå¤©çº¿å¯¹ï¼‰
        x_splits = torch.chunk(x, 18, dim=1)  # 18ä¸ª [batch_size, 30, 20]

        # åˆ›å»ºresizeå‡½æ•° - FIX: Add antialias parameter to suppress warning
        torch_resize = Resize([60, 32], antialias=True)

        # å¤„ç†æ‰€æœ‰6ä¸ªå¤©çº¿å¯¹
        x_resized = []
        for i in range(18):
            x_part = x_splits[i]  # [batch_size, 60, 20]
            # æ·»åŠ channelç»´åº¦
            x_part = x_part.unsqueeze(1)  # [batch_size, 1, 60, 20]
            # Resizeåˆ°æœŸæœ›çš„å°ºå¯¸
            x_part = torch_resize(x_part)  # [batch_size, 1, 60, 32]
            x_resized.append(x_part)

        time_start = time.time()

        # å¯¹æ‰€æœ‰6ä¸ªå¤©çº¿å¯¹è¿›è¡Œç¼–ç 
        encoded_features = []

        for x_input in x_resized:
            # Initial convolution layer
            x_feat = self.encoder_conv1_p1(x_input)  # [batch_size, 64, 60, 32]
            x_feat = self.encoder_bn1_p1(x_feat)
            x_feat = self.encoder_relu_p1(x_feat)

            # ResNet34 layers
            x_feat = self.encoder_layer1_p1(x_feat)  # [batch_size, 64, 60, 32]
            x_feat = self.encoder_layer2_p1(x_feat)  # [batch_size, 128, 30, 16]
            x_feat = self.encoder_layer3_p1(x_feat)  # [batch_size, 256, 15, 8]
            # x_feat = self.encoder_layer4_p1(x_feat)  # [batch_size, 512, 15, 4]

            encoded_features.append(x_feat)

        # Concatenation step
        # Concatenate features from 6 antenna pairs along width dimension
        # Each x_i: [batch_size, 512, 15, 4]
        # Result: [batch_size, 512, 15, 24] (4*6=24 in width dimension)
        x = torch.cat(encoded_features, dim=3)

        # Batch normalization before transformer
        x = self.bn2(x)  # [batch_size, 512, 15, 72]

        # Channel Transformer
        # Input: [batch_size, 512, 15, 24] -> Output: [batch_size, 512, 15, 24]
        x, weight = self.tf(x)

        # Decoder
        # Input: [batch_size, 512, 15, 24] -> Output: [batch_size, 2, 15, 24]
        x = self.decode(x)

        # Average pooling
        # Pool across width dimension: [batch_size, 2, 15, 24] -> [batch_size, 2, 15, 1]
        m = torch.nn.AvgPool2d((1, 144), stride=(1, 1))
        x = m(x).squeeze(dim=3)  # [batch_size, 2, 15]

        # Final batch normalization
        x = self.bn1(x)

        time_end = time.time()
        time_sum = time_end - time_start

        # Transpose to get final pose format
        # [batch_size, 2, 15] -> [batch_size, 15, 2]
        x = torch.transpose(x, 1, 2)

        # è¾“å‡ºé€‚é…ï¼šåªä¿ç•™å‰15ä¸ªå…³é”®ç‚¹
        # x = x[:, :15, :]  # [batch_size, 15, 2]

        return x, time_sum


def weights_init(m):
    """
    Initialize network weights

    Args:
        m: Neural network module to initialize
    """
    if isinstance(m, nn.Conv2d):
        # Xavier normal initialization for convolutional layers
        nn.init.xavier_normal_(m.weight.data)
        # Note: bias initialization is commented out
        # nn.init.xavier_normal_(m.bias.data)
    elif isinstance(m, nn.BatchNorm2d):
        # Constant initialization for 2D batch normalization
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.BatchNorm1d):
        # Constant initialization for 1D batch normalization
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)


# åœ¨è¯„ä¼°å‡½æ•°ä¹‹å‰æ·»åŠ ï¼š
def mean_keypoint_error(pred, target, keypoint_scale=1.0):
    """
    è®¡ç®—å¹³å‡å…³é”®ç‚¹è¯¯å·® (Mean Pose Error)

    Args:
        pred: é¢„æµ‹çš„å…³é”®ç‚¹ [batch_size, 15, 2]
        target: çœŸå®çš„å…³é”®ç‚¹ [batch_size, 15, 2]
        keypoint_scale: ç¼©æ”¾å› å­

    Returns:
        mpe: å¹³å‡å…³é”®ç‚¹è¯¯å·®
    """
    # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
    if len(pred.shape) == 3 and pred.shape[1] == 2 and pred.shape[2] == 15:
        pred = pred.transpose(1, 2)  # [batch_size, 15, 2]
    if len(target.shape) == 3 and target.shape[1] == 2 and target.shape[2] == 15:
        target = target.transpose(1, 2)  # [batch_size, 15, 2]

    # è®¡ç®—æ¬§æ°è·ç¦»
    distances = torch.sqrt(torch.sum((pred - target) ** 2, dim=2))  # [batch_size, 15]

    # è®¡ç®—å¹³å‡è¯¯å·®
    mpe = torch.mean(distances) * keypoint_scale

    return mpe.item()

def extract_keypoints_from_pam(pam_data, num_keypoints=15):
    """
    ä»PAMçŸ©é˜µä¸­æå–å…³é”®ç‚¹åæ ‡å’Œç½®ä¿¡åº¦ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    è¾“å…¥: pam_data [batch_size, 3, 15, 15] - PAMçš„x'ã€y'å’Œç½®ä¿¡åº¦é€šé“
    è¾“å‡º:
        keypoints [batch_size, num_keypoints, 2] - æå–çš„å…³é”®ç‚¹åæ ‡
        confidence [batch_size, num_keypoints, 1] - æå–çš„ç½®ä¿¡åº¦
    """
    batch_size = pam_data.shape[0]

    # åˆå§‹åŒ–å…³é”®ç‚¹åæ ‡å’Œç½®ä¿¡åº¦
    keypoints = torch.zeros(batch_size, num_keypoints, 2, device=pam_data.device)
    confidence = torch.zeros(batch_size, num_keypoints, 1, device=pam_data.device)

    # ä»å¯¹è§’çº¿å…ƒç´ æå–åæ ‡å’Œç½®ä¿¡åº¦
    for b in range(batch_size):
        for k in range(num_keypoints):
            keypoints[b, k, 0] = pam_data[b, 0, k, k]  # xåæ ‡ä»ç¬¬0ä¸ªé€šé“çš„å¯¹è§’çº¿
            keypoints[b, k, 1] = pam_data[b, 1, k, k]  # yåæ ‡ä»ç¬¬1ä¸ªé€šé“çš„å¯¹è§’çº¿
            confidence[b, k, 0] = pam_data[b, 2, k, k]  # ç½®ä¿¡åº¦ä»ç¬¬2ä¸ªé€šé“çš„å¯¹è§’çº¿

    return keypoints, confidence


# ============================== #
# å§¿æ€å¯è§†åŒ–ç›¸å…³è®¾ç½®
# ============================== #
SKELETON_CONNECTIONS = [
    (0, 1), (1, 8),
    (1, 2), (2, 3), (3, 4),
    (1, 5), (5, 6), (6, 7),
    (8, 9), (8, 12),
    (9, 10), (10, 11),
    (12, 13), (13, 14)
]

BODY_PART_COLORS = {
    'head': 'magenta',
    'torso': 'red',
    'left_arm': 'orange',
    'right_arm': 'green',
    'left_leg': 'cyan',
    'right_leg': 'blue'
}

KEYPOINT_GROUPS = {
    'head': [0],
    'torso': [0, 1, 8],
    'left_arm': [2, 3, 4],
    'right_arm': [5, 6, 7],
    'left_leg': [9, 10, 11],
    'right_leg': [12, 13, 14]
}

CONNECTION_COLORS = {
    (0, 1): BODY_PART_COLORS['torso'], (1, 8): BODY_PART_COLORS['torso'],
    (1, 2): BODY_PART_COLORS['left_arm'], (2, 3): BODY_PART_COLORS['left_arm'], (3, 4): BODY_PART_COLORS['left_arm'],
    (1, 5): BODY_PART_COLORS['right_arm'], (5, 6): BODY_PART_COLORS['right_arm'], (6, 7): BODY_PART_COLORS['right_arm'],
    (8, 9): BODY_PART_COLORS['left_leg'], (8, 12): BODY_PART_COLORS['right_leg'],
    (9, 10): BODY_PART_COLORS['left_leg'], (10, 11): BODY_PART_COLORS['left_leg'],
    (12, 13): BODY_PART_COLORS['right_leg'], (13, 14): BODY_PART_COLORS['right_leg']
}


def create_pose_animation_opencv(all_keypoints, output_file="pose_animation.mp4", fps=30,
                                 figsize=(800, 960), keypoint_scale=1000.0):
    """ä½¿ç”¨OpenCVåˆ›å»ºäººä½“å§¿æ€åŠ¨ç”»"""
    width, height = figsize
    frames = len(all_keypoints)

    # ç¡®ä¿å…³é”®ç‚¹æ˜¯numpyæ•°ç»„
    if torch.is_tensor(all_keypoints):
        all_keypoints = all_keypoints.cpu().numpy()

    # ç¼©æ”¾å…³é”®ç‚¹
    all_keypoints = all_keypoints * keypoint_scale

    # è®¡ç®—å…¨å±€è¾¹ç•Œ
    all_x = all_keypoints[:, :, 0].flatten()
    all_y = all_keypoints[:, :, 1].flatten()

    x_min, x_max = np.min(all_x), np.max(all_x)
    y_min, y_max = np.min(all_y), np.max(all_y)

    margin = 0.1
    x_margin = (x_max - x_min) * margin
    y_margin = (y_max - y_min) * margin

    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))

    print(f"ç”Ÿæˆå§¿æ€åŠ¨ç”»: {output_file}ï¼Œå…± {frames} å¸§")

    with tqdm(total=frames, desc="ç”Ÿæˆè§†é¢‘", unit="å¸§") as pbar:
        for frame_idx in range(frames):
            # åˆ›å»ºç™½è‰²èƒŒæ™¯
            frame_img = np.ones((height, width, 3), dtype=np.uint8) * 255

            # åˆ›å»ºmatplotlibå›¾
            fig, ax = plt.subplots(figsize=(width / 100, height / 100), dpi=100)

            keypoints = all_keypoints[frame_idx]

            # ç»˜åˆ¶éª¨éª¼è¿æ¥
            for connection in SKELETON_CONNECTIONS:
                start_idx, end_idx = connection
                color = CONNECTION_COLORS.get(connection, 'gray')
                ax.plot([keypoints[start_idx, 0], keypoints[end_idx, 0]],
                        [keypoints[start_idx, 1], keypoints[end_idx, 1]],
                        color=color, linewidth=3)

            # ç»˜åˆ¶å…³é”®ç‚¹
            for part_name, indices in KEYPOINT_GROUPS.items():
                color = BODY_PART_COLORS[part_name]
                part_keypoints = keypoints[indices]
                ax.scatter(part_keypoints[:, 0], part_keypoints[:, 1],
                           c=color, s=50, edgecolors='black')

            # è®¾ç½®åæ ‡è½´
            ax.set_xlim(x_min - x_margin, x_max + x_margin)
            ax.set_ylim(y_max + y_margin, y_min - y_margin)
            ax.set_title(f"Frame {frame_idx + 1}/{frames}", fontsize=14)
            ax.set_aspect('equal')
            ax.axis('off')

            plt.tight_layout()

            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            fig.canvas.draw()
            mat_img = np.array(fig.canvas.renderer.buffer_rgba())
            mat_img = cv2.cvtColor(mat_img, cv2.COLOR_RGBA2BGR)

            # å†™å…¥è§†é¢‘
            video_writer.write(mat_img)
            plt.close(fig)
            pbar.update(1)

    video_writer.release()
    print(f"è§†é¢‘ç”Ÿæˆå®Œæˆ: {output_file}")
    return output_file


def create_side_by_side_video_opencv(true_keypoints, pred_keypoints, output_file="comparison.mp4",
                                     keypoint_scale=1000.0, fps=30):
    """åˆ›å»ºå¯¹æ¯”è§†é¢‘"""
    frames = min(len(true_keypoints), len(pred_keypoints))

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if torch.is_tensor(true_keypoints):
        true_keypoints = true_keypoints.cpu().numpy()
    if torch.is_tensor(pred_keypoints):
        pred_keypoints = pred_keypoints.cpu().numpy()

    # ç¼©æ”¾å…³é”®ç‚¹
    true_keypoints = true_keypoints[:frames] * keypoint_scale
    pred_keypoints = pred_keypoints[:frames] * keypoint_scale

    # è®¡ç®—å…¨å±€èŒƒå›´
    all_x = np.concatenate([true_keypoints[:, :, 0].flatten(), pred_keypoints[:, :, 0].flatten()])
    all_y = np.concatenate([true_keypoints[:, :, 1].flatten(), pred_keypoints[:, :, 1].flatten()])

    x_min, x_max = np.min(all_x), np.max(all_x)
    y_min, y_max = np.min(all_y), np.max(all_y)

    margin = 0.1
    x_margin = (x_max - x_min) * margin
    y_margin = (y_max - y_min) * margin

    width, height = 1600, 800
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))

    print(f"ç”Ÿæˆå¯¹æ¯”è§†é¢‘: {output_file}ï¼Œå…± {frames} å¸§")

    with tqdm(total=frames, desc="ç”Ÿæˆå¯¹æ¯”è§†é¢‘", unit="å¸§") as pbar:
        for frame_idx in range(frames):
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

            # çœŸå®å§¿æ€
            true_kp = true_keypoints[frame_idx]
            for connection in SKELETON_CONNECTIONS:
                start_idx, end_idx = connection
                color = CONNECTION_COLORS.get(connection, 'gray')
                ax1.plot([true_kp[start_idx, 0], true_kp[end_idx, 0]],
                         [true_kp[start_idx, 1], true_kp[end_idx, 1]],
                         color=color, linewidth=3)

            for part_name, indices in KEYPOINT_GROUPS.items():
                color = BODY_PART_COLORS[part_name]
                part_keypoints = true_kp[indices]
                ax1.scatter(part_keypoints[:, 0], part_keypoints[:, 1],
                            c=color, s=50, edgecolors='black')

            ax1.set_xlim(x_min - x_margin, x_max + x_margin)
            ax1.set_ylim(y_max + y_margin, y_min - y_margin)
            ax1.set_title(f"True Pose - Frame {frame_idx + 1}", fontsize=14)
            ax1.set_aspect('equal')
            ax1.axis('off')

            # é¢„æµ‹å§¿æ€
            pred_kp = pred_keypoints[frame_idx]
            for connection in SKELETON_CONNECTIONS:
                start_idx, end_idx = connection
                color = CONNECTION_COLORS.get(connection, 'gray')
                ax2.plot([pred_kp[start_idx, 0], pred_kp[end_idx, 0]],
                         [pred_kp[start_idx, 1], pred_kp[end_idx, 1]],
                         color=color, linewidth=3)

            for part_name, indices in KEYPOINT_GROUPS.items():
                color = BODY_PART_COLORS[part_name]
                part_keypoints = pred_kp[indices]
                ax2.scatter(part_keypoints[:, 0], part_keypoints[:, 1],
                            c=color, s=50, edgecolors='black')

            ax2.set_xlim(x_min - x_margin, x_max + x_margin)
            ax2.set_ylim(y_max + y_margin, y_min - y_margin)
            ax2.set_title(f"Predicted Pose - Frame {frame_idx + 1}", fontsize=14)
            ax2.set_aspect('equal')
            ax2.axis('off')

            plt.tight_layout()

            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            fig.canvas.draw()
            mat_img = np.array(fig.canvas.renderer.buffer_rgba())
            mat_img = cv2.cvtColor(mat_img, cv2.COLOR_RGBA2BGR)

            video_writer.write(mat_img)
            plt.close(fig)
            pbar.update(1)

    video_writer.release()
    print(f"å¯¹æ¯”è§†é¢‘ç”Ÿæˆå®Œæˆ: {output_file}")
    return output_file


# ============================== #
# è®­ç»ƒå‡½æ•°
# ============================== #
def get_gpu_memory_map():
    """è·å–æ‰€æœ‰GPUçš„æ˜¾å­˜ä¿¡æ¯"""
    result = {}
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3  # è½¬æ¢ä¸ºGB
            result[i] = gpu_memory
    return result


def calculate_optimal_batch_size(gpu_id):
    """æ ¹æ®GPUæ˜¾å­˜è®¡ç®—æœ€ä¼˜æ‰¹é‡å¤§å°"""
    if not torch.cuda.is_available():
        return 32

    # è·å–GPUæ˜¾å­˜ï¼ˆGBï¼‰
    gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory / 1024 ** 3

    # æ ¹æ®æ˜¾å­˜å¤§å°è®¾ç½®æ‰¹é‡å¤§å°
    # ç»éªŒå…¬å¼ï¼šæ¯GBæ˜¾å­˜å¤§çº¦å¯ä»¥å¤„ç†64ä¸ªæ ·æœ¬
    if gpu_memory > 40:  # 4090 (49GB)
        return 128  # æ›´å¤§çš„æ‰¹é‡
    elif gpu_memory > 20:  # 2080Ti (22GB)
        return 128
    elif gpu_memory > 10:
        return 32
    else:
        return 32


def setup_distributed(gpu_ids):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if len(gpu_ids) > 1:
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, gpu_ids))

        if not dist.is_initialized():
            dist.init_process_group(backend='nccl', world_size=len(gpu_ids), rank=0)
        return True
    return False


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def train_posenet():
    # ============================== #
    # ç»Ÿä¸€è¾“å‡ºç›®å½•é…ç½®
    # ============================== #
    # é¢„å¤„ç†æ•°æ®ç›®å½•
    csi_data_dir = "preprocessed_csi_data"
    pam_label_dir = "keypoints_pam_data"  # PAMæ ¼å¼æ ‡ç­¾ç›®å½•

    # ç»Ÿä¸€è¾“å‡ºç›®å½•é…ç½®
    output_dir = "metafi_fix2"  # ä¸»è¾“å‡ºç›®å½•
    model_dir = os.path.join(output_dir, "models")  # æ¨¡å‹æƒé‡ç›®å½•
    video_dir = os.path.join(output_dir, "videos")  # è§†é¢‘è¾“å‡ºç›®å½•
    log_dir = os.path.join(output_dir, "logs")  # æ—¥å¿—ç›®å½•
    result_dir = os.path.join(output_dir, "results")  # ç»“æœæ–‡ä»¶ç›®å½•

    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    directories = [output_dir, model_dir, video_dir, log_dir, result_dir]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {directory}")

    # æ–‡ä»¶è·¯å¾„é…ç½®
    best_model_path = os.path.join(model_dir, 'posenet_best.pth')
    latest_model_path = os.path.join(model_dir, 'posenet_latest.pth')

    true_poses_video = os.path.join(video_dir, 'posenet_true_poses.mp4')
    predicted_poses_video = os.path.join(video_dir, 'posenet_predicted_poses.mp4')
    comparison_video = os.path.join(video_dir, 'posenet_comparison.mp4')

    test_results_file = os.path.join(result_dir, 'test_results.json')
    training_curves_file = os.path.join(result_dir, 'training_curves.png')

    print(f"è¾“å‡ºç›®å½•ç»“æ„å·²åˆ›å»ºå®Œæˆ: {output_dir}")

    # ============================== #
    # è®¾å¤‡é…ç½®
    # ============================== #
    gpu_config = '0'  # æˆ–è€…ä½ æƒ³ä½¿ç”¨çš„GPUé…ç½®
    if gpu_config == 'auto':
        gpu_memory_map = get_gpu_memory_map()
        rtx4090_ids = [i for i, mem in gpu_memory_map.items() if mem > 40]

        if rtx4090_ids:
            gpu_ids = rtx4090_ids[:1]
            print(f"è‡ªåŠ¨é€‰æ‹©: ä½¿ç”¨RTX 4090 (GPU {gpu_ids[0]})")
        else:
            gpu_ids = list(range(torch.cuda.device_count()))
            print(f"è‡ªåŠ¨é€‰æ‹©: ä½¿ç”¨æ‰€æœ‰GPU {gpu_ids}")
    else:
        gpu_ids = [int(x) for x in gpu_config.split(',')]

    print(f"ä½¿ç”¨GPU: {gpu_ids}")

    # æ‰“å°æ¯ä¸ªGPUçš„ä¿¡æ¯
    for gpu_id in gpu_ids:
        props = torch.cuda.get_device_properties(gpu_id)
        print(f"  GPU {gpu_id}: {props.name}, æ˜¾å­˜: {props.total_memory / 1024 ** 3:.1f}GB")

    # è®¾ç½®ä¸»è®¾å¤‡
    device = torch.device(f"cuda:{gpu_ids[0]}" if torch.cuda.is_available() else "cpu")
    torch.cuda.set_device(device)

    # æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    use_ddp = False  # å¼ºåˆ¶ä½¿ç”¨DataParallel
    if use_ddp:
        setup_distributed(gpu_ids)

    print(f"Using device: {device}")

    # ============================== #
    # æ•°æ®åŠ è½½
    # ============================== #
    keypoint_scale = 1000.0

    # æ‰¹é‡å¤§å°
    batch_size = 32

    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csi_data_dir) or not os.path.exists(os.path.join(csi_data_dir, "csi_windows.npy")):
        print(f"é”™è¯¯: æœªæ‰¾åˆ°CSIé¢„å¤„ç†æ•°æ® {csi_data_dir}")
        return

    if not os.path.exists(pam_label_dir):
        print(f"é”™è¯¯: æœªæ‰¾åˆ°PAMæ ‡ç­¾ç›®å½• {pam_label_dir}")
        return

    # åˆ›å»ºæ•°æ®é›†
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    full_dataset = PreprocessedCSIKeypointsDataset(
        csi_data_dir=csi_data_dir,
        pam_label_dir=pam_label_dir,
        keypoint_scale=1000.0,  # æ·»åŠ è¿™ä¸€è¡Œ
        enable_zero_clean=True  # å¯ç”¨é›¶å€¼æ¸…ç†
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = create_preprocessed_train_val_test_loaders(
        dataset=full_dataset,
        batch_size=batch_size,
        num_workers=0
    )

    # ============================== #
    # æ¨¡å‹åˆå§‹åŒ–
    # ============================== #
    model = posenet()
    model.apply(weights_init)
    model = model.to(device)

    # ========== æç®€æ¨¡å‹ç»Ÿè®¡ï¼ˆåªè¦è¿™å‡ è¡Œï¼‰ ==========
    # 1. è®¡ç®—æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“Š æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,} ({total_params / 1e6:.2f}M)")

    # 2. è®¡ç®—FLOPs
    try:
        from thop import profile
        import copy
        model_copy = copy.deepcopy(model)
        input_tensor = torch.randn(1, 540, 20).to(device)
        with torch.no_grad():
            flops, _ = profile(model_copy, inputs=(input_tensor,), verbose=False)
        print(f"ğŸ’» æ¨¡å‹è®¡ç®—é‡: {flops / 1e6:.2f}M FLOPs")
        del model_copy
    except:
        print("ğŸ’» FLOPsè®¡ç®—éœ€è¦å®‰è£…: pip install thop")

    # è®¾ç½®å¹¶è¡Œè®­ç»ƒ
    if use_ddp:
        model = DDP(model, device_ids=gpu_ids, output_device=gpu_ids[0])
        print("ä½¿ç”¨DistributedDataParallel")
    elif len(gpu_ids) > 1:
        model = nn.DataParallel(model, device_ids=gpu_ids)
        print("ä½¿ç”¨DataParallel")

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion_L2 = nn.MSELoss().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    n_epochs = 20
    n_epochs_decay = 30
    epoch_count = 1

    def lambda_rule(epoch):
        lr_l = 1.0 - max(0, epoch + epoch_count - n_epochs) / float(n_epochs_decay + 1)
        return lr_l

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)

    # ============================== #
    # è®­ç»ƒå¾ªç¯
    # ============================== #
    num_epochs = 50
    pck_50_overall_max = 0
    train_mean_loss_iter = []

    print("Starting training...")

    for epoch_index in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss_iter = []

        # æ·»åŠ è®­ç»ƒè¿›åº¦æ¡
        train_pbar = tqdm(enumerate(train_loader), total=len(train_loader),
                          desc=f'Epoch {epoch_index + 1}/{num_epochs} [Train]',
                          unit='batch')

        for idx, (csi_data, pam_batch) in train_pbar:
            csi_data = csi_data.to(device)
            pam_label = pam_batch.to(device)

            # å‰å‘ä¼ æ’­
            pred_xy_keypoint, time_forward = model(csi_data)

            # ä»PAMæ ‡ç­¾æå–çœŸå®å…³é”®ç‚¹
            keypoints, confidence = extract_keypoints_from_pam(pam_label)  # pam_labelæ˜¯å®Œæ•´çš„[B, 3, 15, 15]

            # è®¡ç®—æŸå¤±
            loss = criterion_L2(torch.mul(confidence, pred_xy_keypoint), torch.mul(confidence, keypoints))

            train_loss_iter.append(loss.cpu().detach().numpy())

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            lr = scheduler.get_last_lr()[0]
            processed_samples = (idx + 1) * batch_size

            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºä¿¡æ¯
            train_pbar.set_postfix({
                'loss': f'{loss:.4f}',
                'lr': f'{lr:.5f}',
                'samples': f'{processed_samples}/{len(train_loader.dataset)}'
            })

        train_pbar.close()

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        train_mean_loss = np.mean(train_loss_iter)
        train_mean_loss_iter.append(train_mean_loss)
        print('end of the epoch: %d, with loss: %.3f' % (epoch_index, train_mean_loss))

        # ============================== #
        # éªŒè¯é˜¶æ®µ
        # ============================== #
        model.eval()
        valid_loss_iter = []
        pck_50_iter = []
        pck_20_iter = []
        mpe_iter = []

        # æ·»åŠ éªŒè¯è¿›åº¦æ¡
        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch_index + 1}/{num_epochs} [Valid]',
                        unit='batch', leave=False)

        with torch.no_grad():
            for csi_data, pam_batch in val_pbar:
                csi_data = csi_data.to(device)
                pam_label = pam_batch.to(device)

                # å‰å‘ä¼ æ’­
                pred_xy_keypoint, time_forward = model(csi_data)

                # ä»PAMæ ‡ç­¾æå–çœŸå®å…³é”®ç‚¹
                keypoints, confidence = extract_keypoints_from_pam(pam_label)  # pam_labelæ˜¯å®Œæ•´çš„[B, 3, 15, 15]

                # è®¡ç®—æŸå¤±
                loss = criterion_L2(torch.mul(confidence, pred_xy_keypoint), torch.mul(confidence, keypoints))

                valid_loss_iter.append(loss.cpu().detach().numpy())

                # è®¡ç®—MPE
                mpe = mean_keypoint_error(pred_xy_keypoint, keypoints, keypoint_scale)
                mpe_iter.append(mpe)

                # è®¡ç®—PCK - éœ€è¦è½¬æ¢æ ¼å¼
                pred_xy_keypoint_pck = pred_xy_keypoint.transpose(1, 2)
                keypoints_pck = keypoints.transpose(1, 2)

                pck_50 = compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.5)
                pck_20 = compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.2)

                pck_50_iter.append(pck_50)
                pck_20_iter.append(pck_20)

                # æ›´æ–°éªŒè¯è¿›åº¦æ¡
                val_pbar.set_postfix({'val_loss': f'{loss:.4f}'})

        val_pbar.close()

        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        valid_mean_loss = np.mean(valid_loss_iter)
        valid_mean_mpe = np.mean(mpe_iter)
        pck_50 = np.mean(pck_50_iter, 0)
        pck_20 = np.mean(pck_20_iter, 0)

        pck_50_overall = pck_50[-1] if len(pck_50.shape) > 0 else pck_50
        pck_20_overall = pck_20[-1] if len(pck_20.shape) > 0 else pck_20

        print('validation result with loss: %.3f, mpe: %.3f, pck_50: %.3f, pck_20: %.3f' %
              (valid_mean_loss, valid_mean_mpe, pck_50_overall, pck_20_overall))

        # ============================== #
        # ä¿å­˜æ¨¡å‹ - ä½¿ç”¨ç»Ÿä¸€è·¯å¾„é…ç½®
        # ============================== #
        if pck_50_overall > pck_50_overall_max:
            print('saving the model at the end of epoch %d with pck_50: %.3f' %
                  (epoch_index, pck_50_overall))
            torch.save(model, best_model_path)
            pck_50_overall_max = pck_50_overall

        # å®šæœŸä¿å­˜æœ€æ–°æ¨¡å‹
        if (epoch_index + 1) % 10 == 0:
            torch.save(model, latest_model_path)

        # å®šæœŸæ‰“å°è®­ç»ƒå†å²
        if (epoch_index + 1) % 50 == 0:
            print('the train loss for the first %.1f epoch is' % (epoch_index))
            print(train_mean_loss_iter)

    print("\nTraining completed!")

    # ============================== #
    # æµ‹è¯•é˜¶æ®µ
    # ============================== #
    print("\nStarting testing...")

    # åŠ è½½æœ€ä½³æ¨¡å‹ - ä½¿ç”¨ç»Ÿä¸€è·¯å¾„é…ç½®
    model = torch.load(best_model_path, map_location=device)
    model = model.to(device).eval()

    test_loss_iter = []
    pck_50_iter = []
    pck_40_iter = []
    pck_30_iter = []
    pck_20_iter = []
    pck_10_iter = []
    pck_5_iter = []
    mpe_iter = []

    # ä¿å­˜æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å…³é”®ç‚¹ç”¨äºè§†é¢‘ç”Ÿæˆ
    all_pred_keypoints = []
    all_true_keypoints = []

    # æ·»åŠ æµ‹è¯•è¿›åº¦æ¡
    test_pbar = tqdm(test_loader, desc='Testing', unit='batch')

    with torch.no_grad():
        for csi_data, pam_batch in test_pbar:
            csi_data = csi_data.to(device)
            pam_label = pam_batch.to(device)

            # å‰å‘ä¼ æ’­
            pred_xy_keypoint, time_forward = model(csi_data)

            # ä»PAMæ ‡ç­¾æå–çœŸå®å…³é”®ç‚¹
            keypoints, confidence = extract_keypoints_from_pam(pam_label)  # pam_labelæ˜¯å®Œæ•´çš„[B, 3, 15, 15]

            # è®¡ç®—æŸå¤±
            loss = criterion_L2(torch.mul(confidence, pred_xy_keypoint), torch.mul(confidence, keypoints))

            test_loss_iter.append(loss.cpu().detach().numpy())

            # æ·»åŠ MPEè®¡ç®—
            mpe = mean_keypoint_error(pred_xy_keypoint, keypoints, keypoint_scale)
            mpe_iter.append(mpe)

            # ä¿å­˜é¢„æµ‹å’ŒçœŸå®å…³é”®ç‚¹
            all_pred_keypoints.append(pred_xy_keypoint.cpu().numpy())
            all_true_keypoints.append(keypoints.cpu().numpy())

            # è®¡ç®—ä¸åŒé˜ˆå€¼çš„PCK
            pred_xy_keypoint_pck = pred_xy_keypoint.transpose(1, 2)
            keypoints_pck = keypoints.transpose(1, 2)

            pck_50_iter.append(compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.5))
            pck_40_iter.append(compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.4))
            pck_30_iter.append(compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.3))
            pck_20_iter.append(compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.2))
            pck_10_iter.append(compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.1))
            pck_5_iter.append(compute_pck_pckh_15(pred_xy_keypoint_pck.cpu(), keypoints_pck.cpu(), 0.05))

            # æ›´æ–°æµ‹è¯•è¿›åº¦æ¡
            test_pbar.set_postfix({
                'test_loss': f'{loss:.4f}',
                'time': f'{time_forward:.3f}s'
            })

    test_pbar.close()

    # ============================== #
    # è®¡ç®—æµ‹è¯•ç»“æœ
    # ============================== #
    test_mean_loss = np.mean(test_loss_iter)
    test_mean_mpe = np.mean(mpe_iter)
    pck_50 = np.mean(pck_50_iter, 0)
    pck_40 = np.mean(pck_40_iter, 0)
    pck_30 = np.mean(pck_30_iter, 0)
    pck_20 = np.mean(pck_20_iter, 0)
    pck_10 = np.mean(pck_10_iter, 0)
    pck_5 = np.mean(pck_5_iter, 0)

    # è·å–overall PCK
    pck_50_overall = pck_50[-1] if len(pck_50.shape) > 0 else pck_50
    pck_40_overall = pck_40[-1] if len(pck_40.shape) > 0 else pck_40
    pck_30_overall = pck_30[-1] if len(pck_30.shape) > 0 else pck_30
    pck_20_overall = pck_20[-1] if len(pck_20.shape) > 0 else pck_20
    pck_10_overall = pck_10[-1] if len(pck_10.shape) > 0 else pck_10
    pck_5_overall = pck_5[-1] if len(pck_5.shape) > 0 else pck_5

    print('test result with loss: %.3f, mpe: %.3f, pck_50: %.3f, pck_40: %.3f, pck_30: %.3f, '
          'pck_20: %.3f, pck_10: %.3f, pck_5: %.3f' %
          (test_mean_loss, test_mean_mpe, pck_50_overall, pck_40_overall, pck_30_overall,
           pck_20_overall, pck_10_overall, pck_5_overall))

    print('-----pck_50-----')
    print(pck_50)
    print('-----pck_40-----')
    print(pck_40)
    print('-----pck_30-----')
    print(pck_30)
    print('-----pck_20-----')
    print(pck_20)
    print('-----pck_10-----')
    print(pck_10)
    print('-----pck_5-----')
    print(pck_5)

    # ============================== #
    # ä¿å­˜æµ‹è¯•ç»“æœ - ä½¿ç”¨ç»Ÿä¸€è·¯å¾„é…ç½®
    # ============================== #
    test_results = {
        'test_loss': float(test_mean_loss),
        'test_mpe': float(test_mean_mpe),
        'pck_50': float(pck_50_overall),
        'pck_40': float(pck_40_overall),
        'pck_30': float(pck_30_overall),
        'pck_20': float(pck_20_overall),
        'pck_10': float(pck_10_overall),
        'pck_5': float(pck_5_overall),
        'detailed_pck': {
            'pck_50_per_joint': pck_50.tolist() if hasattr(pck_50, 'tolist') else [float(pck_50)],
            'pck_40_per_joint': pck_40.tolist() if hasattr(pck_40, 'tolist') else [float(pck_40)],
            'pck_30_per_joint': pck_30.tolist() if hasattr(pck_30, 'tolist') else [float(pck_30)],
            'pck_20_per_joint': pck_20.tolist() if hasattr(pck_20, 'tolist') else [float(pck_20)],
            'pck_10_per_joint': pck_10.tolist() if hasattr(pck_10, 'tolist') else [float(pck_10)],
            'pck_5_per_joint': pck_5.tolist() if hasattr(pck_5, 'tolist') else [float(pck_5)]
        },
        'training_config': {
            'num_epochs': num_epochs,
            'batch_size': batch_size,
            'keypoint_scale': keypoint_scale,
            'learning_rate': 0.001
        }
    }

    import json
    with open(test_results_file, 'w') as f:
        json.dump(test_results, f, indent=2)

    print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_results_file}")

    # ============================== #
    # ç”Ÿæˆè§†é¢‘ - ä½¿ç”¨ç»Ÿä¸€è·¯å¾„é…ç½®
    # ============================== #
    if len(all_pred_keypoints) > 0:
        print("\nç”Ÿæˆå¯è§†åŒ–è§†é¢‘...")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_pred_keypoints = np.vstack(all_pred_keypoints)
        all_true_keypoints = np.vstack(all_true_keypoints)

        # é€‰æ‹©ä¸€éƒ¨åˆ†å¸§æ¥ç”Ÿæˆè§†é¢‘ï¼ˆä¾‹å¦‚å‰720å¸§ï¼‰
        frames_to_animate = min(720, len(all_pred_keypoints))

        # ç”ŸæˆçœŸå®å§¿æ€è§†é¢‘
        print("ç”ŸæˆçœŸå®å§¿æ€è§†é¢‘...")
        create_pose_animation_opencv(
            all_true_keypoints[:frames_to_animate],
            output_file=true_poses_video,
            keypoint_scale=keypoint_scale
        )

        # ç”Ÿæˆé¢„æµ‹å§¿æ€è§†é¢‘
        print("ç”Ÿæˆé¢„æµ‹å§¿æ€è§†é¢‘...")
        create_pose_animation_opencv(
            all_pred_keypoints[:frames_to_animate],
            output_file=predicted_poses_video,
            keypoint_scale=keypoint_scale
        )

        # ç”Ÿæˆå¯¹æ¯”è§†é¢‘
        print("ç”Ÿæˆå¯¹æ¯”è§†é¢‘...")
        create_side_by_side_video_opencv(
            all_true_keypoints[:frames_to_animate],
            all_pred_keypoints[:frames_to_animate],
            output_file=comparison_video,
            keypoint_scale=keypoint_scale
        )

        print("\nè§†é¢‘ç”Ÿæˆå®Œæˆï¼")
        print(f"çœŸå®å§¿æ€è§†é¢‘: {true_poses_video}")
        print(f"é¢„æµ‹å§¿æ€è§†é¢‘: {predicted_poses_video}")
        print(f"å¯¹æ¯”è§†é¢‘: {comparison_video}")

    # ============================== #
    # ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾ - ä½¿ç”¨ç»Ÿä¸€è·¯å¾„é…ç½®
    # ============================== #
    try:
        import matplotlib.pyplot as plt

        plt.figure(figsize=(10, 6))
        plt.plot(train_mean_loss_iter, label='Training Loss', color='blue')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Loss Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(training_curves_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {training_curves_file}")
    except Exception as e:
        print(f"ä¿å­˜è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")

    print(f"\næ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")
    print("è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    train_posenet()